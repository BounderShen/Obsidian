#### 设计详述
##### 
在我们的需求中设计文件系统时，我们有以下假设，这些假设既带来了挑战，也带来了机会。我们之前提到了一些关键的观察结果，现在将这些假设详细说明：
- 系统由许多廉价的通用组件构建，这些组件经常发生故障。它必须不断监视自己，并及时检测、容忍和恢复组件故障。
- 系统存储了一些大文件。我们期望有几百万个文件，每个文件通常为100 MB或更大。多GB文件是常见情况，应该有效地管理。需要支持小文件，但无需针对它们进行优化。
- 工作负载主要包括两种读取方式：大型流式读取和小型随机读取。在大型流式读取中，每个操作通常读取数百KB，更常见的是1 MB或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移量处读取几KB。对性能敏感的应用程序通常会对其小型读取进行批处理和排序，以便稳定地通过文件前进，而不是来回跳动。
- 工作负载还包括许多大的顺序写入，将数据追加到文件中。典型的操作大小与读取相似。一旦写入，文件很少再次修改。支持文件中任意位置的小型写入，但不必高效。
- 系统必须有效地实现多个客户端的良好定义的语义，这些客户端同时追加到同一文件中。我们的文件经常用作生产者-消费者队列或多路合并。数百个生产者将同时在一台机器上运行，追加到一个文件中。原子性与最小化的同步开销是必不可少的。文件以后可能会被读取，或者消费者可能同时浏览文件。
- 高持续带宽比低延迟更重要。我们的大多数目标应用程序都强调以高速率批量处理数据，而对于单个读取或写入没有严格的响应时间要求。

GFS提供了一个熟悉的文件系统界面，尽管它没有实现标准的API，如POSIX。文件以目录层次结构组织，并通过路径名进行标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。

此外，GFS还具有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时将数据追加到同一文件中，同时保证每个单独客户端的追加的原子性。它对于实现多路合并结果和生产者-消费者队列非常有用，许多客户端可以同时追加而无需额外锁定。我们发现这些类型的文件在构建大型分布式应用程序中非常有价值。快照和记录追加在3.4和3.3节中进一步讨论。

##### 架构
GFS集群由单个主服务器和多个块服务器组成，并由多个客户端访问，如图1所示。每个服务器通常是运行用户级服务器进程的通用Linux机器。只要机器资源允许并且运行可能不稳定的应用程序代码所导致的较低的可靠性可接受，那么在同一台机器上运行块服务器和客户端非常容易。

文件被分成固定大小的块。每个块由主服务器在块创建时分配的不可变且全局唯一的64位块句柄来标识。块服务器将块存储为Linux文件的本地磁盘，并读取或写入由块句柄和字节范围指定的块数据。为了可靠性，每个块在多个块服务器上复制。默认情况下，我们存储三个副本，尽管用户可以为文件命名空间的不同区域指定不同的复制级别。

主服务器维护所有文件系统元数据。这包括命名空间、访问控制信息、文件到块的映射以及块的当前位置。它还控制系统范围的活动，例如块租赁管理、孤立块的垃圾收集和块在块服务器之间的迁移。主服务器定期在心跳消息中与每个块服务器通信，以给出指令和收集其状态。

链接到每个应用程序的GFS客户端代码实现了文件系统API，并与主服务器和块服务器通信，代表应用程序读取或写入数据。客户端与主服务器进行元数据操作的交互，但所有携带数据的通信直接发送到块服务器。我们不提供POSIX API，因此无需连接到Linux vnode层。

客户端和块服务器都不缓存文件数据。客户端缓存提供的好处很少，因为大多数应用程序都流式传输大文件或具有太大的工作集而无法缓存。不使用客户端缓存通过消除缓存一致性问题简化了客户端和整个系统。（但客户端会缓存元数据。）块服务器不需要缓存文件数据，因为块作为本地文件存储，因此Linux的缓冲区缓存已经将经常访问的数据保留在内存中。

##### 单主服务器
拥有单个主服务器极大地简化了我们的设计，并使主服务器能够使用全局知识做出复杂的块放置和复制决策。但是，我们必须最大限度地减少其在读写中的参与，以防止其成为瓶颈。客户端永远不会通过主服务器读取和写入文件数据。相反，客户端询问主服务器应该联系哪些块服务器。它会缓存这些信息一段有限的时间，并直接与块服务器进行许多后续操作。

让我们通过参考图1中的简单读取来解释交互。首先，使用固定的块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件中的块索引。然后，它向主服务器发送一个请求，其中包含文件名和块索引。主服务器回复相应的块句柄和副本的位置。客户端使用文件名和块索引作为键缓存这些信息。然后，客户端向其中一个副本发送请求，很可能是最近的一个。请求指定块句柄和该块内的字节范围。对于相同块的进一步读取，不需要更多的客户端-主服务器交互，直到缓存的信息过期或文件重新打开。实际上，客户端通常在同一个请求中请求多个块，主服务器也可以包括请求后面的块的信息。这些额外的信息几乎不需要额外成本，可避免几个将来的客户端-主服务器交互。

##### 块大小
块大小是关键的设计参数之一。我们选择了64 MB，这比典型的文件系统块大小大得多。每个块副本都存储为块服务器上的普通Linux文件，并且只在需要时进行扩展。惰性空间分配避免了由于内部碎片而浪费空间，这也许是对这么大的块大小最大的反对意见。

大块大小提供了几个重要的优点。首先，它减少了客户端与主服务器交互的需要，因为对同一块的读取和写入仅需要向主服务器发出一次初始请求以获取块位置信息。这种减少对于我们的工作负载尤为重要，因为应用程序大多顺序读写大文件。即使是小的随机读取，客户端也可以轻松地缓存所有多TB工作集的块位置信息。其次，由于在大块上，客户端更有可能在给定的块上执行许多操作，因此它可以通过在长时间内保持与块服务器的持久TCP连接来减少网络开销。第三，它减少了存储在主服务器上的元数据的大小。这使我们能够将元数据保持在内存中，这进一步带来了其他优点，我们将在第2.6.1节中讨论。

另一方面，即使使用惰性空间分配，大的块大小也有其缺点。小文件由少量块组成，可能只有一个块。如果许多客户端正在访问同一个文件，则存储这些块的块服务器可能会成为热点。实际上，热点并没有成为主要问题，因为我们的应用程序大多顺序读取大的多块文件。

然而，在GFS首次被批处理队列系统使用时，热点问题确实出现了：一个可执行文件被写入GFS作为单个块文件，然后在同时启动数百台机器上运行。存储这个可执行文件的少数块服务器会被数百个同时请求过载。我们通过使用更高的复制因子存储这些可执行文件并使批处理队列系统交错应用程序启动时间来解决了这个问题。一个潜在的长期解决方案是在这种情况下允许客户端从其他客户端读取数据。

##### 元数据
主服务器存储三种主要类型的元数据：文件和块名称空间、文件到块的映射以及每个块副本的位置。所有的元数据都保存在主服务器的内存中。前两种类型（名称空间和文件到块的映射）也通过记录操作日志中的变异来保持持久性，该操作日志存储在主服务器的本地磁盘上并在远程机器上复制。使用日志可以使我们简单、可靠地更新主服务器状态，而不会在主服务器崩溃时出现不一致性。主服务器不持久地存储块位置信息。相反，它在主服务器启动时和每当块服务器加入集群时询问每个块服务器有关其块的信息。

###### 2.6.1 内存数据结构
由于元数据存储在内存中，主服务器操作非常快速。而且，主服务器定期在后台扫描整个状态非常容易和高效。这种定期扫描用于实现块垃圾回收，在块服务器故障的情况下重新复制块，以及块迁移以平衡负载和磁盘空间使用率。第4.3节和第4.4节将进一步讨论这些活动。

这种仅使用内存的方法的一个潜在问题是，块的数量和整个系统的容量受主服务器的内存大小限制。但实际上这不是一个严重的限制。主服务器维护每个64 MB块不到64字节的元数据。大多数块都是满的，因为大多数文件包含许多块，只有最后一个块可能是部分填充的。同样，文件名称空间数据通常需要少于64字节的空间，因为它使用前缀压缩紧凑地存储文件名。

如果需要支持更大的文件系统，增加主服务器的额外内存成本是一个小代价，因为我们通过将元数据存储在内存中获得了简单、可靠、高性能和灵活性。

###### 块位置
主服务器不会持久地记录哪些块服务器具有给定块的副本。它只是在启动时轮询块服务器以获取该信息。之后，主服务器可以通过使用定期的心跳消息来控制所有块的放置位置并监视块服务器的状态，使自己保持最新。

我们最初尝试在主服务器上持久地保留块位置信息，但我们决定在启动时从块服务器请求数据，之后定期请求。这消除了在块服务器加入和离开集群、更改名称、失败、重新启动等情况下保持主服务器和块服务器同步的问题。在拥有数百个服务器的集群中，这些事件经常发生。

理解这个设计决策的另一种方法是意识到块服务器对它自己的磁盘上有哪些块有或没有有最终决定权。试图在主服务器上维护此信息的一致视图没有意义，因为块服务器上的错误可能会导致块自发消失（例如，磁盘可能损坏并被禁用），或者操作员可能会重命名块服务器。

###### 操作日志
操作日志包含关键元数据更改的历史记录。它是GFS的核心。它不仅是元数据的唯一持久记录，而且还作为定义并发操作顺序的逻辑时间线。文件和块以及它们的版本（见第4.5节）都通过它们被创建的逻辑时间唯一且永久地标识。

由于操作日志非常重要，我们必须可靠地存储它，并且在元数据更改变得持久之前不向客户端公开更改。否则，即使块本身幸存，我们也会失去整个文件系统或最近的客户端操作。因此，我们在多个远程计算机上复制它，并且仅在将相应的日志记录刷新到本地磁盘和远程磁盘后才响应客户端操作。主服务器在刷新之前会将多个日志记录一起批处理，从而减少刷新和复制对整个系统吞吐量的影响。

主服务器通过重放操作日志来恢复其文件系统状态。为了使启动时间最小化，我们必须使日志保持小。主服务器在日志增长到一定大小时对其状态进行检查点，以便它可以通过从本地磁盘加载最新检查点并仅重放之后的有限数量的日志记录来恢复。检查点采用紧凑的类B树形式，可以直接映射到内存中并用于命名空间查找，而无需额外解析。这进一步加快了恢复速度并提高了可用性。

由于建立检查点可能需要一段时间，主服务器的内部状态被结构化为可以在不延迟传入变异的情况下创建新检查点。主服务器切换到一个新的日志文件并在单独的线程中创建新的检查点。新检查点包括切换之前的所有变异。对于拥有几百万个文件的集群，它可以在一分钟左右创建完成。完成后，它会被写入本地磁盘和远程磁盘。

恢复仅需要最新的完整检查点和随后的日志文件。旧检查点和日志文件可以自由删除，尽管我们会保留一些用于防范灾难。在检查点过程中发生故障不会影响正确性，
##### 2.7一致性模型
GFS 的一致性模型是松散的，这意味着它能够很好地支持高度分布式的应用程序，同时保持实现相对简单和高效。GFS 为应用程序提供了一些保证，这些保证在本节中进行了讨论。
###### 2.7.1保证byGFS 
文件命名空间的变化（例如文件创建）是原子的，并且由主服务器独占处理。命名空间锁定保证了原子性和正确性，而主服务器的操作日志定义了这些操作的全局总顺序。

数据变异后的文件区域状态取决于变异的类型，是否成功或失败以及是否存在并发变异。如果变异在没有并发写入的情况下成功，则受影响的区域是定义和一致的，这意味着所有客户端始终会看到变异所写的内容。同时成功的变异会使区域变得未定义但一致，这意味着所有客户端看到相同的数据，但它可能不反映任何一个变异所写的内容。失败的变异会使区域不一致，不同的客户端可能在不同的时间看到不同的数据。

数据变异可以是写入或记录追加。写入会在应用程序指定的文件偏移量处写入数据，而记录追加会使数据（即“记录”）至少被追加一次，即使存在并发变异也是如此。在一系列成功的变异之后，变异的文件区域保证被定义并包含最后一个变异所写的数据。这是通过在所有副本上以相同的顺序应用变异，并使用块版本号检测任何因其块服务器关闭而错过变异的副本来实现的。

GFS 通过主服务器和所有块服务器之间的定期握手来识别失败的块服务器，并通过校验和检测数据损坏。一旦出现问题，数据将尽快从有效的副本中恢复，并且只有在 GFS 无法做出反应的情况下，所有副本都丢失后，块才会不可逆地丢失。即使在这种情况下，它也变得不可用，而不是损坏，应用程序会收到明确的错误，而不是损坏的数据。
###### 应用实现
该段文字讨论了记录追加在文件I/O操作中的两种典型用法。在第一种情况下，写入者随着数据变得可用将记录附加到文件中，导致从应用程序的角度看，文件数据仍不完整。在第二种情况下，多个写入者并发地向一个文件附加，以获取合并结果或作为生产者-消费者队列。记录追加的至少一次附加语义保留了每个写入者的输出。读者通过以下方式处理偶尔的填充和重复项。每个写入者准备的每个记录都包含额外的信息，例如校验和，以便可以验证其有效性。读者可以使用校验和识别和丢弃额外的填充和记录片段。如果读者不能容忍偶尔的重复项（例如，如果它们会触发不幂等的操作），则可以使用记录中的唯一标识符将其过滤掉，这通常也需要用于命名相应的应用程序实体，例如Web文档。这些记录I/O的功能（除了重复项的删除）都在我们应用程序中共享的库代码中，并适用于Google的其他文件接口实现。因此，相同的记录序列加上稀有的重复项始终会传递给记录读取器。
