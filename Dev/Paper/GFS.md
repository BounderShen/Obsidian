#### 1介绍
我们设计和实现了Google文件系统（GFS），以满足Google日益增长的数据处理需求。GFS与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。然而，它的设计是基于我们的应用工作负载和技术环境的关键观察，这反映出与一些早期文件系统设计假设的明显不同。我们重新审视了传统的选择，并在设计空间中探索了根本不同的点。

首先，组件故障是常态而不是例外。文件系统由数百甚至数千个由廉价的商品部件构建的存储机器组成，并由相当数量的客户机器访问。组件的数量和质量几乎保证在任何给定时间都有一些组件不起作用，有些组件将无法从它们当前的故障中恢复。我们已经看到由应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源供应的故障引起的问题。因此，不断的监控、错误检测、容错和自动恢复必须成为系统的组成部分。

第二，文件按传统标准非常巨大。多GB文件很常见。每个文件通常包含许多应用程序对象，如Web文档。当我们定期处理许多TB的快速增长数据集，其中包括数十亿个对象时，即使文件系统可以支持，管理数十亿个大小约为KB的文件也是不方便的。因此，必须重新审视设计假设和参数，如I/O操作和块大小，以优化大文件大小和相应的工作负载特征。

第三，流式读取和写入是大文件的主要访问模式。工作负载通常涉及大量数据集的批处理或分布式数据密集型计算。在这些场景中，数据通常从开始到结束顺序读取或从结尾开始顺序写入。因此，GFS优化了流式读取和写入，而不是更传统的随机访问模式。

第四，高持续带宽比低延迟更重要。因为工作负载涉及大文件和流式访问模式，吞吐量是主要的性能指标，持续带宽比延迟更重要。此观察引导了一种设计，它优化了大的、顺序的I/O操作，而不是小的、随机的I/O操作。

最后，共同设计应用程序和文件系统通常会导致更好的整体性能和功能。GFS旨在与各种数据密集型应用程序一起使用，我们发现共同设计这些应用程序和文件系统可以带来显着的性能优势。例如，GFS提供原子记录追加操作的支持，这使得将数据实时流式写入文件的应用程序能够高效地进行数据摄入。

总体而言，GFS的设计反映了一种与传统文件系统设计假设不同的方法，而是针对Google大规模数据处理的独特工作负载和技术环境进行了优化。

除了覆盖现有数据外，文件内的随机写操作实际上是不存在的。一旦写入，这些文件通常只会被读取，而且通常只是顺序读取。许多不同类型的数据都具有这些特征。有些可能是数据分析程序扫描的大型存储库。有些可能是由运行应用程序不断生成的数据流。有些可能是存档数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时还是随后的时间。鉴于对巨大文件的访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。

第四，共同设计应用程序和文件系统API有助于增加系统的灵活性。例如，我们放松了GFS的一致性模型，大大简化了文件系统而没有对应用程序施加繁重的负担。我们还引入了原子追加操作，使多个客户端可以在不需要额外同步的情况下并发追加到同一个文件中。这些将在本文后面详细讨论。

目前已经部署了多个GFS集群以满足不同的目的。最大的集群拥有1000多个存储节点，超过300 TB的磁盘存储空间，并且由数百个在不同机器上的客户端进行频繁访问。

#### 2设计详述
##### 2.1假设

在我们的需求中设计文件系统时，我们有以下假设，这些假设既带来了挑战，也带来了机会。我们之前提到了一些关键的观察结果，现在将这些假设详细说明：
- 系统由许多廉价的通用组件构建，这些组件经常发生故障。它必须不断监视自己，并及时检测、容忍和恢复组件故障。
- 系统存储了一些大文件。我们期望有几百万个文件，每个文件通常为100 MB或更大。多GB文件是常见情况，应该有效地管理。需要支持小文件，但无需针对它们进行优化。
- 工作负载主要包括两种读取方式：大型流式读取和小型随机读取。在大型流式读取中，每个操作通常读取数百KB，更常见的是1 MB或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移量处读取几KB。对性能敏感的应用程序通常会对其小型读取进行批处理和排序，以便稳定地通过文件前进，而不是来回跳动。
- 工作负载还包括许多大的顺序写入，将数据追加到文件中。典型的操作大小与读取相似。一旦写入，文件很少再次修改。支持文件中任意位置的小型写入，但不必高效。
- 系统必须有效地实现多个客户端的良好定义的语义，这些客户端同时追加到同一文件中。我们的文件经常用作生产者-消费者队列或多路合并。数百个生产者将同时在一台机器上运行，追加到一个文件中。原子性与最小化的同步开销是必不可少的。文件以后可能会被读取，或者消费者可能同时浏览文件。
- 高持续带宽比低延迟更重要。我们的大多数目标应用程序都强调以高速率批量处理数据，而对于单个读取或写入没有严格的响应时间要求。
##### 2.2接口
GFS提供了一个熟悉的文件系统界面，尽管它没有实现标准的API，如POSIX。文件以目录层次结构组织，并通过路径名进行标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。

此外，GFS还具有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时将数据追加到同一文件中，同时保证每个单独客户端的追加的原子性。它对于实现多路合并结果和生产者-消费者队列非常有用，许多客户端可以同时追加而无需额外锁定。我们发现这些类型的文件在构建大型分布式应用程序中非常有价值。快照和记录追加在3.4和3.3节中进一步讨论。

##### 2.3架构

![](E:\IT\Obsidian\img\GFS1.png)

GFS集群由单个主服务器和多个块服务器组成，并由多个客户端访问，如图1所示。每个服务器通常是运行用户级服务器进程的通用Linux机器。只要机器资源允许并且运行可能不稳定的应用程序代码所导致的较低的可靠性可接受，那么在同一台机器上运行块服务器和客户端非常容易。

文件被分成固定大小的块。每个块由主服务器在块创建时分配的不可变且全局唯一的64位块句柄来标识。块服务器将块存储为Linux文件的本地磁盘，并读取或写入由块句柄和字节范围指定的块数据。为了可靠性，每个块在多个块服务器上复制。默认情况下，我们存储三个副本，尽管用户可以为文件命名空间的不同区域指定不同的复制级别。

主服务器维护所有文件系统元数据。这包括命名空间、访问控制信息、文件到块的映射以及块的当前位置。它还控制系统范围的活动，例如块租赁管理、孤立块的垃圾收集和块在块服务器之间的迁移。主服务器定期在心跳消息中与每个块服务器通信，以给出指令和收集其状态。

链接到每个应用程序的GFS客户端代码实现了文件系统API，并与主服务器和块服务器通信，代表应用程序读取或写入数据。客户端与主服务器进行元数据操作的交互，但所有携带数据的通信直接发送到块服务器。我们不提供POSIX API，因此无需连接到Linux vnode层。

客户端和块服务器都不缓存文件数据。客户端缓存提供的好处很少，因为大多数应用程序都流式传输大文件或具有太大的工作集而无法缓存。不使用客户端缓存通过消除缓存一致性问题简化了客户端和整个系统。（但客户端会缓存元数据。）块服务器不需要缓存文件数据，因为块作为本地文件存储，因此Linux的缓冲区缓存已经将经常访问的数据保留在内存中。

##### 2.4单主服务器
拥有单个主服务器极大地简化了我们的设计，并使主服务器能够使用全局知识做出复杂的块放置和复制决策。但是，我们必须最大限度地减少其在读写中的参与，以防止其成为瓶颈。客户端永远不会通过主服务器读取和写入文件数据。相反，客户端询问主服务器应该联系哪些块服务器。它会缓存这些信息一段有限的时间，并直接与块服务器进行许多后续操作。

让我们通过参考图1中的简单读取来解释交互。首先，使用固定的块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件中的块索引。然后，它向主服务器发送一个请求，其中包含文件名和块索引。主服务器回复相应的块句柄和副本的位置。客户端使用文件名和块索引作为键缓存这些信息。然后，客户端向其中一个副本发送请求，很可能是最近的一个。请求指定块句柄和该块内的字节范围。对于相同块的进一步读取，不需要更多的客户端-主服务器交互，直到缓存的信息过期或文件重新打开。实际上，客户端通常在同一个请求中请求多个块，主服务器也可以包括请求后面的块的信息。这些额外的信息几乎不需要额外成本，可避免几个将来的客户端-主服务器交互。

##### 2.5块大小
块大小是关键的设计参数之一。我们选择了64 MB，这比典型的文件系统块大小大得多。每个块副本都存储为块服务器上的普通Linux文件，并且只在需要时进行扩展。惰性空间分配避免了由于内部碎片而浪费空间，这也许是对这么大的块大小最大的反对意见。

大块大小提供了几个重要的优点。首先，它减少了客户端与主服务器交互的需要，因为对同一块的读取和写入仅需要向主服务器发出一次初始请求以获取块位置信息。这种减少对于我们的工作负载尤为重要，因为应用程序大多顺序读写大文件。即使是小的随机读取，客户端也可以轻松地缓存所有多TB工作集的块位置信息。其次，由于在大块上，客户端更有可能在给定的块上执行许多操作，因此它可以通过在长时间内保持与块服务器的持久TCP连接来减少网络开销。第三，它减少了存储在主服务器上的元数据的大小。这使我们能够将元数据保持在内存中，这进一步带来了其他优点，我们将在第2.6.1节中讨论。

另一方面，即使使用惰性空间分配，大的块大小也有其缺点。小文件由少量块组成，可能只有一个块。如果许多客户端正在访问同一个文件，则存储这些块的块服务器可能会成为热点。实际上，热点并没有成为主要问题，因为我们的应用程序大多顺序读取大的多块文件。

然而，在GFS首次被批处理队列系统使用时，热点问题确实出现了：一个可执行文件被写入GFS作为单个块文件，然后在同时启动数百台机器上运行。存储这个可执行文件的少数块服务器会被数百个同时请求过载。我们通过使用更高的复制因子存储这些可执行文件并使批处理队列系统交错应用程序启动时间来解决了这个问题。一个潜在的长期解决方案是在这种情况下允许客户端从其他客户端读取数据。

##### 2.6元数据
主服务器存储三种主要类型的元数据：文件和块名称空间、文件到块的映射以及每个块副本的位置。所有的元数据都保存在主服务器的内存中。前两种类型（名称空间和文件到块的映射）也通过记录操作日志中的变异来保持持久性，该操作日志存储在主服务器的本地磁盘上并在远程机器上复制。使用日志可以使我们简单、可靠地更新主服务器状态，而不会在主服务器崩溃时出现不一致性。主服务器不持久地存储块位置信息。相反，它在主服务器启动时和每当块服务器加入集群时询问每个块服务器有关其块的信息。

###### 2.6.1 内存数据结构
由于元数据存储在内存中，主服务器操作非常快速。而且，主服务器定期在后台扫描整个状态非常容易和高效。这种定期扫描用于实现块垃圾回收，在块服务器故障的情况下重新复制块，以及块迁移以平衡负载和磁盘空间使用率。第4.3节和第4.4节将进一步讨论这些活动。

这种仅使用内存的方法的一个潜在问题是，块的数量和整个系统的容量受主服务器的内存大小限制。但实际上这不是一个严重的限制。主服务器维护每个64 MB块不到64字节的元数据。大多数块都是满的，因为大多数文件包含许多块，只有最后一个块可能是部分填充的。同样，文件名称空间数据通常需要少于64字节的空间，因为它使用前缀压缩紧凑地存储文件名。

如果需要支持更大的文件系统，增加主服务器的额外内存成本是一个小代价，因为我们通过将元数据存储在内存中获得了简单、可靠、高性能和灵活性。

###### 2.6.2块位置
主服务器不会持久地记录哪些块服务器具有给定块的副本。它只是在启动时轮询块服务器以获取该信息。之后，主服务器可以通过使用定期的心跳消息来控制所有块的放置位置并监视块服务器的状态，使自己保持最新。

我们最初尝试在主服务器上持久地保留块位置信息，但我们决定在启动时从块服务器请求数据，之后定期请求。这消除了在块服务器加入和离开集群、更改名称、失败、重新启动等情况下保持主服务器和块服务器同步的问题。在拥有数百个服务器的集群中，这些事件经常发生。

理解这个设计决策的另一种方法是意识到块服务器对它自己的磁盘上有哪些块有或没有有最终决定权。试图在主服务器上维护此信息的一致视图没有意义，因为块服务器上的错误可能会导致块自发消失（例如，磁盘可能损坏并被禁用），或者操作员可能会重命名块服务器。

###### 2.6.3操作日志
操作日志包含关键元数据更改的历史记录。它是GFS的核心。它不仅是元数据的唯一持久记录，而且还作为定义并发操作顺序的逻辑时间线。文件和块以及它们的版本（见第4.5节）都通过它们被创建的逻辑时间唯一且永久地标识。

由于操作日志非常重要，我们必须可靠地存储它，并且在元数据更改变得持久之前不向客户端公开更改。否则，即使块本身幸存，我们也会失去整个文件系统或最近的客户端操作。因此，我们在多个远程计算机上复制它，并且仅在将相应的日志记录刷新到本地磁盘和远程磁盘后才响应客户端操作。主服务器在刷新之前会将多个日志记录一起批处理，从而减少刷新和复制对整个系统吞吐量的影响。

主服务器通过重放操作日志来恢复其文件系统状态。为了使启动时间最小化，我们必须使日志保持小。主服务器在日志增长到一定大小时对其状态进行检查点，以便它可以通过从本地磁盘加载最新检查点并仅重放之后的有限数量的日志记录来恢复。检查点采用紧凑的类B树形式，可以直接映射到内存中并用于命名空间查找，而无需额外解析。这进一步加快了恢复速度并提高了可用性。

由于建立检查点可能需要一段时间，主服务器的内部状态被结构化为可以在不延迟传入变异的情况下创建新检查点。主服务器切换到一个新的日志文件并在单独的线程中创建新的检查点。新检查点包括切换之前的所有变异。对于拥有几百万个文件的集群，它可以在一分钟左右创建完成。完成后，它会被写入本地磁盘和远程磁盘。

恢复仅需要最新的完整检查点和随后的日志文件。旧检查点和日志文件可以自由删除，尽管我们会保留一些用于防范灾难。在检查点过程中发生故障不会影响正确性，
##### 2.7一致性模型
GFS 的一致性模型是松散的，这意味着它能够很好地支持高度分布式的应用程序，同时保持实现相对简单和高效。GFS 为应用程序提供了一些保证，这些保证在本节中进行了讨论。
###### 2.7.1保证byGFS 
文件命名空间的变化（例如文件创建）是原子的，并且由主服务器独占处理。命名空间锁定保证了原子性和正确性，而主服务器的操作日志定义了这些操作的全局总顺序。

数据变异后的文件区域状态取决于变异的类型，是否成功或失败以及是否存在并发变异。如果变异在没有并发写入的情况下成功，则受影响的区域是定义和一致的，这意味着所有客户端始终会看到变异所写的内容。同时成功的变异会使区域变得未定义但一致，这意味着所有客户端看到相同的数据，但它可能不反映任何一个变异所写的内容。失败的变异会使区域不一致，不同的客户端可能在不同的时间看到不同的数据。

数据变异可以是写入或记录追加。写入会在应用程序指定的文件偏移量处写入数据，而记录追加会使数据（即“记录”）至少被追加一次，即使存在并发变异也是如此。在一系列成功的变异之后，变异的文件区域保证被定义并包含最后一个变异所写的数据。这是通过在所有副本上以相同的顺序应用变异，并使用块版本号检测任何因其块服务器关闭而错过变异的副本来实现的。

GFS 通过主服务器和所有块服务器之间的定期握手来识别失败的块服务器，并通过校验和检测数据损坏。一旦出现问题，数据将尽快从有效的副本中恢复，并且只有在 GFS 无法做出反应的情况下，所有副本都丢失后，块才会不可逆地丢失。即使在这种情况下，它也变得不可用，而不是损坏，应用程序会收到明确的错误，而不是损坏的数据。
###### 2.7.2应用实现
该段文字讨论了记录追加在文件I/O操作中的两种典型用法。在第一种情况下，写入者随着数据变得可用将记录附加到文件中，导致从应用程序的角度看，文件数据仍不完整。在第二种情况下，多个写入者并发地向一个文件附加，以获取合并结果或作为生产者-消费者队列。记录追加的至少一次附加语义保留了每个写入者的输出。读者通过以下方式处理偶尔的填充和重复项。每个写入者准备的每个记录都包含额外的信息，例如校验和，以便可以验证其有效性。读者可以使用校验和识别和丢弃额外的填充和记录片段。如果读者不能容忍偶尔的重复项（例如，如果它们会触发不幂等的操作），则可以使用记录中的唯一标识符将其过滤掉，这通常也需要用于命名相应的应用程序实体，例如Web文档。这些记录I/O的功能（除了重复项的删除）都在我们应用程序中共享的库代码中，并适用于Google的其他文件接口实现。因此，相同的记录序列加上稀有的重复项始终会传递给记录读取器。
#### 3系统交互

我们设计了系统，以最小化主节点在所有操作中的参与。在这个背景下，我们现在描述客户端、主节点和块服务器如何相互作用，以实现数据变异、原子记录追加和快照功能。
##### 3.1租约和变异顺序

![](E:\IT\Obsidian\img\GFS2.png)

租约和变异顺序

变异是指改变块内容或元数据的操作，例如写或追加操作。每个变异都在所有副本上执行。我们使用租约来维护一致的变异顺序。主节点授予一个块租约给其中一个副本，我们称之为主副本。主副本为所有变异选择一个序列顺序。所有副本在应用变异时都遵循这个顺序。因此，全局变异顺序首先由主节点选择的租约授予顺序定义，在租约内部由主副本分配的序列号定义。

租约机制旨在最小化主节点的管理开销。租约的初始超时时间为60秒。但只要块正在进行变异，主副本就可以无限期地请求并通常获得主节点的扩展。这些扩展请求和授权会被打包在主节点和所有块服务器之间定期交换的心跳消息中。

主节点有时可能会在租约过期之前尝试撤销租约（例如，当主节点希望禁用正在重命名的文件上的变异时）。即使主节点失去了与主副本的通信，它也可以在旧租约过期后安全地向另一个副本授予新租约。

在图2中，我们通过跟随这些编号步骤来说明写入的控制流程：

1. 客户端询问主节点哪个块服务器拥有当前块的租约以及其他副本的位置。如果没有人拥有租约，主节点将租约授予它选择的一个副本（未显示）。
2. 主节点回复具有主副本身份和其他（次要）副本位置的标识。客户端缓存此数据以进行将来的变异。只有当主副本变得无法访问或回复它不再持有租约时，客户端才需要再次联系主节点。
3. 客户端将数据推送到所有副本。客户端可以以任意顺序这样做。每个块服务器将在内部LRU缓冲区缓存数据，直到数据被使用或过期。通过将数据流与控制流解耦，我们可以通过基于网络拓扑调度昂贵的数据流来改善性能，而不管哪个块服务器是主副本。第3.2节进一步讨论了这一点。

4. 当所有副本都确认接收到数据后，客户端向主副本发送写请求。该请求标识了先前推送到所有副本的数据。主副本为接收到的所有变异（可能来自多个客户端）分配连续的序列号，提供必要的序列化。它按照序列号顺序将变异应用于其自己的本地状态。

5. 主副本将写请求转发给所有次要副本。每个次要副本按主副本分配的相同序列号顺序应用变异。

6. 次要副本都向主副本回复，表明它们已完成操作。

7. 主副本向客户端回复。任何在副本中遇到的错误都会向客户端报告。如果发生错误，写入可能已在主副本和任意子集的次要副本中成功完成。如果在主副本中失败，它就不会被分配序列号并转发。客户端请求被认为失败，并且修改的区域处于不一致状态。我们的客户端代码通过重试失败的变异来处理此类错误。在开始写入之前，它将尝试步骤（3）到（7）几次，然后回退到重新尝试。
如果应用程序的写入大小较大或跨越了块边界，GFS客户端代码将其分解为多个写操作。它们都遵循上面描述的控制流程，但可能与其他客户端的并发操作交错和覆盖。因此，共享文件区域可能最终包含来自不同客户端的片段，尽管副本将是相同的，因为所有副本上的单个操作都按相同顺序成功完成。如第2.7节所述，这使文件区域处于一致但未定义的状态。
##### 3.2数据流
我们将数据流与控制流分离，以有效地利用网络。虽然控制流从客户端流向主副本，然后流向所有次要副本，但数据是以流水线方式沿着精心选择的块服务器链线性推送的。我们的目标是充分利用每台机器的网络带宽，避免网络瓶颈和高延迟链接，并尽可能地缩短推送所有数据的延迟时间。

为了充分利用每台机器的网络带宽，数据沿着块服务器链线性推送，而不是分布在其他拓扑结构（例如树形结构）中。因此，将每台机器的全部出站带宽用于尽可能快地传输数据，而不是分配给多个接收者。

为了尽可能避免网络瓶颈和高延迟链接（例如，交换机之间的链接通常都是这样的），每台机器将数据转发到网络拓扑结构中尚未接收数据的“最近”机器。假设客户端正在将数据推送到块服务器S1到S4。它将数据发送到最近的块服务器，例如S1。S1将其转发到最接近S1的块服务器S2到S4，例如S2。类似地，S2将其转发到S3或S4中更接近S2的那个，以此类推。我们的网络拓扑结构足够简单，可以从IP地址准确估计“距离”。

最后，我们通过TCP连接流水线传输数据，以最小化延迟。一旦块服务器接收到一些数据，它就立即开始转发。对我们而言，流水线非常有帮助，因为我们使用具有全双工链接的交换网络。立即发送数据不会降低接收速率。在没有网络拥塞的情况下，将B字节传输到R个副本的理想经过时间为B/T + RL，其中T是网络吞吐量，L是两台机器之间传输字节的延迟。我们的网络链接通常是100 Mbps（T），而L远低于1毫秒。因此，1 MB理论上可以在约80毫秒内分发。
