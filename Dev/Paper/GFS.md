#### 1介绍
我们设计和实现了Google文件系统（GFS），以满足Google日益增长的数据处理需求。GFS与以前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。然而，它的设计是基于我们的应用工作负载和技术环境的关键观察，这反映出与一些早期文件系统设计假设的明显不同。我们重新审视了传统的选择，并在设计空间中探索了根本不同的点。

首先，组件故障是常态而不是例外。文件系统由数百甚至数千个由廉价的商品部件构建的存储机器组成，并由相当数量的客户机器访问。组件的数量和质量几乎保证在任何给定时间都有一些组件不起作用，有些组件将无法从它们当前的故障中恢复。我们已经看到由应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源供应的故障引起的问题。因此，不断的监控、错误检测、容错和自动恢复必须成为系统的组成部分。

第二，文件按传统标准非常巨大。多GB文件很常见。每个文件通常包含许多应用程序对象，如Web文档。当我们定期处理许多TB的快速增长数据集，其中包括数十亿个对象时，即使文件系统可以支持，管理数十亿个大小约为KB的文件也是不方便的。因此，必须重新审视设计假设和参数，如I/O操作和块大小，以优化大文件大小和相应的工作负载特征。

第三，流式读取和写入是大文件的主要访问模式。工作负载通常涉及大量数据集的批处理或分布式数据密集型计算。在这些场景中，数据通常从开始到结束顺序读取或从结尾开始顺序写入。因此，GFS优化了流式读取和写入，而不是更传统的随机访问模式。

第四，高持续带宽比低延迟更重要。因为工作负载涉及大文件和流式访问模式，吞吐量是主要的性能指标，持续带宽比延迟更重要。此观察引导了一种设计，它优化了大的、顺序的I/O操作，而不是小的、随机的I/O操作。

最后，共同设计应用程序和文件系统通常会导致更好的整体性能和功能。GFS旨在与各种数据密集型应用程序一起使用，我们发现共同设计这些应用程序和文件系统可以带来显着的性能优势。例如，GFS提供原子记录追加操作的支持，这使得将数据实时流式写入文件的应用程序能够高效地进行数据摄入。

总体而言，GFS的设计反映了一种与传统文件系统设计假设不同的方法，而是针对Google大规模数据处理的独特工作负载和技术环境进行了优化。

除了覆盖现有数据外，文件内的随机写操作实际上是不存在的。一旦写入，这些文件通常只会被读取，而且通常只是顺序读取。许多不同类型的数据都具有这些特征。有些可能是数据分析程序扫描的大型存储库。有些可能是由运行应用程序不断生成的数据流。有些可能是存档数据。有些可能是在一台机器上生成并在另一台机器上处理的中间结果，无论是同时还是随后的时间。鉴于对巨大文件的访问模式，追加成为性能优化和原子性保证的重点，而在客户端缓存数据块则失去了吸引力。

第四，共同设计应用程序和文件系统API有助于增加系统的灵活性。例如，我们放松了GFS的一致性模型，大大简化了文件系统而没有对应用程序施加繁重的负担。我们还引入了原子追加操作，使多个客户端可以在不需要额外同步的情况下并发追加到同一个文件中。这些将在本文后面详细讨论。

目前已经部署了多个GFS集群以满足不同的目的。最大的集群拥有1000多个存储节点，超过300 TB的磁盘存储空间，并且由数百个在不同机器上的客户端进行频繁访问。

#### 2设计详述
##### 2.1假设

在我们的需求中设计文件系统时，我们有以下假设，这些假设既带来了挑战，也带来了机会。我们之前提到了一些关键的观察结果，现在将这些假设详细说明：
- 系统由许多廉价的通用组件构建，这些组件经常发生故障。它必须不断监视自己，并及时检测、容忍和恢复组件故障。
- 系统存储了一些大文件。我们期望有几百万个文件，每个文件通常为100 MB或更大。多GB文件是常见情况，应该有效地管理。需要支持小文件，但无需针对它们进行优化。
- 工作负载主要包括两种读取方式：大型流式读取和小型随机读取。在大型流式读取中，每个操作通常读取数百KB，更常见的是1 MB或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移量处读取几KB。对性能敏感的应用程序通常会对其小型读取进行批处理和排序，以便稳定地通过文件前进，而不是来回跳动。
- 工作负载还包括许多大的顺序写入，将数据追加到文件中。典型的操作大小与读取相似。一旦写入，文件很少再次修改。支持文件中任意位置的小型写入，但不必高效。
- 系统必须有效地实现多个客户端的良好定义的语义，这些客户端同时追加到同一文件中。我们的文件经常用作生产者-消费者队列或多路合并。数百个生产者将同时在一台机器上运行，追加到一个文件中。原子性与最小化的同步开销是必不可少的。文件以后可能会被读取，或者消费者可能同时浏览文件。
- 高持续带宽比低延迟更重要。我们的大多数目标应用程序都强调以高速率批量处理数据，而对于单个读取或写入没有严格的响应时间要求。
##### 2.2接口
GFS提供了一个熟悉的文件系统界面，尽管它没有实现标准的API，如POSIX。文件以目录层次结构组织，并通过路径名进行标识。我们支持创建、删除、打开、关闭、读取和写入文件的常规操作。

此外，GFS还具有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时将数据追加到同一文件中，同时保证每个单独客户端的追加的原子性。它对于实现多路合并结果和生产者-消费者队列非常有用，许多客户端可以同时追加而无需额外锁定。我们发现这些类型的文件在构建大型分布式应用程序中非常有价值。快照和记录追加在3.4和3.3节中进一步讨论。

##### 2.3架构

![](E:\IT\Obsidian\img\GFS1.png)

GFS集群由单个主服务器和多个块服务器组成，并由多个客户端访问，如图1所示。每个服务器通常是运行用户级服务器进程的通用Linux机器。只要机器资源允许并且运行可能不稳定的应用程序代码所导致的较低的可靠性可接受，那么在同一台机器上运行块服务器和客户端非常容易。

文件被分成固定大小的块。每个块由主服务器在块创建时分配的不可变且全局唯一的64位块句柄来标识。块服务器将块存储为Linux文件的本地磁盘，并读取或写入由块句柄和字节范围指定的块数据。为了可靠性，每个块在多个块服务器上复制。默认情况下，我们存储三个副本，尽管用户可以为文件命名空间的不同区域指定不同的复制级别。

主服务器维护所有文件系统元数据。这包括命名空间、访问控制信息、文件到块的映射以及块的当前位置。它还控制系统范围的活动，例如块租赁管理、孤立块的垃圾收集和块在块服务器之间的迁移。主服务器定期在心跳消息中与每个块服务器通信，以给出指令和收集其状态。

链接到每个应用程序的GFS客户端代码实现了文件系统API，并与主服务器和块服务器通信，代表应用程序读取或写入数据。客户端与主服务器进行元数据操作的交互，但所有携带数据的通信直接发送到块服务器。我们不提供POSIX API，因此无需连接到Linux vnode层。

客户端和块服务器都不缓存文件数据。客户端缓存提供的好处很少，因为大多数应用程序都流式传输大文件或具有太大的工作集而无法缓存。不使用客户端缓存通过消除缓存一致性问题简化了客户端和整个系统。（但客户端会缓存元数据。）块服务器不需要缓存文件数据，因为块作为本地文件存储，因此Linux的缓冲区缓存已经将经常访问的数据保留在内存中。

##### 2.4单主服务器
拥有单个主服务器极大地简化了我们的设计，并使主服务器能够使用全局知识做出复杂的块放置和复制决策。但是，我们必须最大限度地减少其在读写中的参与，以防止其成为瓶颈。客户端永远不会通过主服务器读取和写入文件数据。相反，客户端询问主服务器应该联系哪些块服务器。它会缓存这些信息一段有限的时间，并直接与块服务器进行许多后续操作。

让我们通过参考图1中的简单读取来解释交互。首先，使用固定的块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件中的块索引。然后，它向主服务器发送一个请求，其中包含文件名和块索引。主服务器回复相应的块句柄和副本的位置。客户端使用文件名和块索引作为键缓存这些信息。然后，客户端向其中一个副本发送请求，很可能是最近的一个。请求指定块句柄和该块内的字节范围。对于相同块的进一步读取，不需要更多的客户端-主服务器交互，直到缓存的信息过期或文件重新打开。实际上，客户端通常在同一个请求中请求多个块，主服务器也可以包括请求后面的块的信息。这些额外的信息几乎不需要额外成本，可避免几个将来的客户端-主服务器交互。

##### 2.5块大小
块大小是关键的设计参数之一。我们选择了64 MB，这比典型的文件系统块大小大得多。每个块副本都存储为块服务器上的普通Linux文件，并且只在需要时进行扩展。惰性空间分配避免了由于内部碎片而浪费空间，这也许是对这么大的块大小最大的反对意见。

大块大小提供了几个重要的优点。首先，它减少了客户端与主服务器交互的需要，因为对同一块的读取和写入仅需要向主服务器发出一次初始请求以获取块位置信息。这种减少对于我们的工作负载尤为重要，因为应用程序大多顺序读写大文件。即使是小的随机读取，客户端也可以轻松地缓存所有多TB工作集的块位置信息。其次，由于在大块上，客户端更有可能在给定的块上执行许多操作，因此它可以通过在长时间内保持与块服务器的持久TCP连接来减少网络开销。第三，它减少了存储在主服务器上的元数据的大小。这使我们能够将元数据保持在内存中，这进一步带来了其他优点，我们将在第2.6.1节中讨论。

另一方面，即使使用惰性空间分配，大的块大小也有其缺点。小文件由少量块组成，可能只有一个块。如果许多客户端正在访问同一个文件，则存储这些块的块服务器可能会成为热点。实际上，热点并没有成为主要问题，因为我们的应用程序大多顺序读取大的多块文件。

然而，在GFS首次被批处理队列系统使用时，热点问题确实出现了：一个可执行文件被写入GFS作为单个块文件，然后在同时启动数百台机器上运行。存储这个可执行文件的少数块服务器会被数百个同时请求过载。我们通过使用更高的复制因子存储这些可执行文件并使批处理队列系统交错应用程序启动时间来解决了这个问题。一个潜在的长期解决方案是在这种情况下允许客户端从其他客户端读取数据。

##### 2.6元数据
主服务器存储三种主要类型的元数据：文件和块名称空间、文件到块的映射以及每个块副本的位置。所有的元数据都保存在主服务器的内存中。前两种类型（名称空间和文件到块的映射）也通过记录操作日志中的变异来保持持久性，该操作日志存储在主服务器的本地磁盘上并在远程机器上复制。使用日志可以使我们简单、可靠地更新主服务器状态，而不会在主服务器崩溃时出现不一致性。主服务器不持久地存储块位置信息。相反，它在主服务器启动时和每当块服务器加入集群时询问每个块服务器有关其块的信息。

###### 2.6.1 内存数据结构
由于元数据存储在内存中，主服务器操作非常快速。而且，主服务器定期在后台扫描整个状态非常容易和高效。这种定期扫描用于实现块垃圾回收，在块服务器故障的情况下重新复制块，以及块迁移以平衡负载和磁盘空间使用率。第4.3节和第4.4节将进一步讨论这些活动。

这种仅使用内存的方法的一个潜在问题是，块的数量和整个系统的容量受主服务器的内存大小限制。但实际上这不是一个严重的限制。主服务器维护每个64 MB块不到64字节的元数据。大多数块都是满的，因为大多数文件包含许多块，只有最后一个块可能是部分填充的。同样，文件名称空间数据通常需要少于64字节的空间，因为它使用前缀压缩紧凑地存储文件名。

如果需要支持更大的文件系统，增加主服务器的额外内存成本是一个小代价，因为我们通过将元数据存储在内存中获得了简单、可靠、高性能和灵活性。

###### 2.6.2块位置
主服务器不会持久地记录哪些块服务器具有给定块的副本。它只是在启动时轮询块服务器以获取该信息。之后，主服务器可以通过使用定期的心跳消息来控制所有块的放置位置并监视块服务器的状态，使自己保持最新。

我们最初尝试在主服务器上持久地保留块位置信息，但我们决定在启动时从块服务器请求数据，之后定期请求。这消除了在块服务器加入和离开集群、更改名称、失败、重新启动等情况下保持主服务器和块服务器同步的问题。在拥有数百个服务器的集群中，这些事件经常发生。

理解这个设计决策的另一种方法是意识到块服务器对它自己的磁盘上有哪些块有或没有有最终决定权。试图在主服务器上维护此信息的一致视图没有意义，因为块服务器上的错误可能会导致块自发消失（例如，磁盘可能损坏并被禁用），或者操作员可能会重命名块服务器。

###### 2.6.3操作日志
操作日志包含关键元数据更改的历史记录。它是GFS的核心。它不仅是元数据的唯一持久记录，而且还作为定义并发操作顺序的逻辑时间线。文件和块以及它们的版本（见第4.5节）都通过它们被创建的逻辑时间唯一且永久地标识。

由于操作日志非常重要，我们必须可靠地存储它，并且在元数据更改变得持久之前不向客户端公开更改。否则，即使块本身幸存，我们也会失去整个文件系统或最近的客户端操作。因此，我们在多个远程计算机上复制它，并且仅在将相应的日志记录刷新到本地磁盘和远程磁盘后才响应客户端操作。主服务器在刷新之前会将多个日志记录一起批处理，从而减少刷新和复制对整个系统吞吐量的影响。

主服务器通过重放操作日志来恢复其文件系统状态。为了使启动时间最小化，我们必须使日志保持小。主服务器在日志增长到一定大小时对其状态进行检查点，以便它可以通过从本地磁盘加载最新检查点并仅重放之后的有限数量的日志记录来恢复。检查点采用紧凑的类B树形式，可以直接映射到内存中并用于命名空间查找，而无需额外解析。这进一步加快了恢复速度并提高了可用性。

由于建立检查点可能需要一段时间，主服务器的内部状态被结构化为可以在不延迟传入变异的情况下创建新检查点。主服务器切换到一个新的日志文件并在单独的线程中创建新的检查点。新检查点包括切换之前的所有变异。对于拥有几百万个文件的集群，它可以在一分钟左右创建完成。完成后，它会被写入本地磁盘和远程磁盘。

恢复仅需要最新的完整检查点和随后的日志文件。旧检查点和日志文件可以自由删除，尽管我们会保留一些用于防范灾难。在检查点过程中发生故障不会影响正确性，
##### 2.7一致性模型
GFS 的一致性模型是松散的，这意味着它能够很好地支持高度分布式的应用程序，同时保持实现相对简单和高效。GFS 为应用程序提供了一些保证，这些保证在本节中进行了讨论。
###### 2.7.1保证byGFS 
文件命名空间的变化（例如文件创建）是原子的，并且由主服务器独占处理。命名空间锁定保证了原子性和正确性，而主服务器的操作日志定义了这些操作的全局总顺序。

数据变异后的文件区域状态取决于变异的类型，是否成功或失败以及是否存在并发变异。如果变异在没有并发写入的情况下成功，则受影响的区域是定义和一致的，这意味着所有客户端始终会看到变异所写的内容。同时成功的变异会使区域变得未定义但一致，这意味着所有客户端看到相同的数据，但它可能不反映任何一个变异所写的内容。失败的变异会使区域不一致，不同的客户端可能在不同的时间看到不同的数据。

数据变异可以是写入或记录追加。写入会在应用程序指定的文件偏移量处写入数据，而记录追加会使数据（即“记录”）至少被追加一次，即使存在并发变异也是如此。在一系列成功的变异之后，变异的文件区域保证被定义并包含最后一个变异所写的数据。这是通过在所有副本上以相同的顺序应用变异，并使用块版本号检测任何因其块服务器关闭而错过变异的副本来实现的。

GFS 通过主服务器和所有块服务器之间的定期握手来识别失败的块服务器，并通过校验和检测数据损坏。一旦出现问题，数据将尽快从有效的副本中恢复，并且只有在 GFS 无法做出反应的情况下，所有副本都丢失后，块才会不可逆地丢失。即使在这种情况下，它也变得不可用，而不是损坏，应用程序会收到明确的错误，而不是损坏的数据。
###### 2.7.2应用实现
该段文字讨论了记录追加在文件I/O操作中的两种典型用法。在第一种情况下，写入者随着数据变得可用将记录附加到文件中，导致从应用程序的角度看，文件数据仍不完整。在第二种情况下，多个写入者并发地向一个文件附加，以获取合并结果或作为生产者-消费者队列。记录追加的至少一次附加语义保留了每个写入者的输出。读者通过以下方式处理偶尔的填充和重复项。每个写入者准备的每个记录都包含额外的信息，例如校验和，以便可以验证其有效性。读者可以使用校验和识别和丢弃额外的填充和记录片段。如果读者不能容忍偶尔的重复项（例如，如果它们会触发不幂等的操作），则可以使用记录中的唯一标识符将其过滤掉，这通常也需要用于命名相应的应用程序实体，例如Web文档。这些记录I/O的功能（除了重复项的删除）都在我们应用程序中共享的库代码中，并适用于Google的其他文件接口实现。因此，相同的记录序列加上稀有的重复项始终会传递给记录读取器。
#### 3系统交互

我们设计了系统，以最小化主节点在所有操作中的参与。在这个背景下，我们现在描述客户端、主节点和块服务器如何相互作用，以实现数据变异、原子记录追加和快照功能。
##### 3.1租约和变异顺序

![](E:\IT\Obsidian\img\GFS2.png)

租约和变异顺序

变异是指改变块内容或元数据的操作，例如写或追加操作。每个变异都在所有副本上执行。我们使用租约来维护一致的变异顺序。主节点授予一个块租约给其中一个副本，我们称之为主副本。主副本为所有变异选择一个序列顺序。所有副本在应用变异时都遵循这个顺序。因此，全局变异顺序首先由主节点选择的租约授予顺序定义，在租约内部由主副本分配的序列号定义。

租约机制旨在最小化主节点的管理开销。租约的初始超时时间为60秒。但只要块正在进行变异，主副本就可以无限期地请求并通常获得主节点的扩展。这些扩展请求和授权会被打包在主节点和所有块服务器之间定期交换的心跳消息中。

主节点有时可能会在租约过期之前尝试撤销租约（例如，当主节点希望禁用正在重命名的文件上的变异时）。即使主节点失去了与主副本的通信，它也可以在旧租约过期后安全地向另一个副本授予新租约。

在图2中，我们通过跟随这些编号步骤来说明写入的控制流程：

1. 客户端询问主节点哪个块服务器拥有当前块的租约以及其他副本的位置。如果没有人拥有租约，主节点将租约授予它选择的一个副本（未显示）。
2. 主节点回复具有主副本身份和其他（次要）副本位置的标识。客户端缓存此数据以进行将来的变异。只有当主副本变得无法访问或回复它不再持有租约时，客户端才需要再次联系主节点。
3. 客户端将数据推送到所有副本。客户端可以以任意顺序这样做。每个块服务器将在内部LRU缓冲区缓存数据，直到数据被使用或过期。通过将数据流与控制流解耦，我们可以通过基于网络拓扑调度昂贵的数据流来改善性能，而不管哪个块服务器是主副本。第3.2节进一步讨论了这一点。

4. 当所有副本都确认接收到数据后，客户端向主副本发送写请求。该请求标识了先前推送到所有副本的数据。主副本为接收到的所有变异（可能来自多个客户端）分配连续的序列号，提供必要的序列化。它按照序列号顺序将变异应用于其自己的本地状态。

5. 主副本将写请求转发给所有次要副本。每个次要副本按主副本分配的相同序列号顺序应用变异。

6. 次要副本都向主副本回复，表明它们已完成操作。

7. 主副本向客户端回复。任何在副本中遇到的错误都会向客户端报告。如果发生错误，写入可能已在主副本和任意子集的次要副本中成功完成。如果在主副本中失败，它就不会被分配序列号并转发。客户端请求被认为失败，并且修改的区域处于不一致状态。我们的客户端代码通过重试失败的变异来处理此类错误。在开始写入之前，它将尝试步骤（3）到（7）几次，然后回退到重新尝试。
如果应用程序的写入大小较大或跨越了块边界，GFS客户端代码将其分解为多个写操作。它们都遵循上面描述的控制流程，但可能与其他客户端的并发操作交错和覆盖。因此，共享文件区域可能最终包含来自不同客户端的片段，尽管副本将是相同的，因为所有副本上的单个操作都按相同顺序成功完成。如第2.7节所述，这使文件区域处于一致但未定义的状态。
##### 3.2数据流
我们将数据流与控制流分离，以有效地利用网络。虽然控制流从客户端流向主副本，然后流向所有次要副本，但数据是以流水线方式沿着精心选择的块服务器链线性推送的。我们的目标是充分利用每台机器的网络带宽，避免网络瓶颈和高延迟链接，并尽可能地缩短推送所有数据的延迟时间。

为了充分利用每台机器的网络带宽，数据沿着块服务器链线性推送，而不是分布在其他拓扑结构（例如树形结构）中。因此，将每台机器的全部出站带宽用于尽可能快地传输数据，而不是分配给多个接收者。

为了尽可能避免网络瓶颈和高延迟链接（例如，交换机之间的链接通常都是这样的），每台机器将数据转发到网络拓扑结构中尚未接收数据的“最近”机器。假设客户端正在将数据推送到块服务器S1到S4。它将数据发送到最近的块服务器，例如S1。S1将其转发到最接近S1的块服务器S2到S4，例如S2。类似地，S2将其转发到S3或S4中更接近S2的那个，以此类推。我们的网络拓扑结构足够简单，可以从IP地址准确估计“距离”。

最后，我们通过TCP连接流水线传输数据，以最小化延迟。一旦块服务器接收到一些数据，它就立即开始转发。对我们而言，流水线非常有帮助，因为我们使用具有全双工链接的交换网络。立即发送数据不会降低接收速率。在没有网络拥塞的情况下，将B字节传输到R个副本的理想经过时间为B/T + RL，其中T是网络吞吐量，L是两台机器之间传输字节的延迟。我们的网络链接通常是100 Mbps（T），而L远低于1毫秒。因此，1 MB理论上可以在约80毫秒内分发。
##### 3.3原子记录追加
GFS提供了一种称为记录追加（record append）的原子追加操作。在传统的写操作中，客户端指定要写入数据的偏移量。对同一区域的并发写操作不是可串行化的：该区域最终可能包含来自多个客户端的数据片段。但是，在记录追加中，客户端只指定数据。GFS将其原子地追加到文件中至少一次（即作为一个连续的字节序列）在GFS所选择的偏移量处，并将该偏移量返回给客户端。这类似于在Unix中以O APPEND模式打开文件进行写入，而没有多个写入者并发时的竞争条件。在我们的分布式应用程序中，记录追加被广泛用于许多不同机器上的客户端同时追加到同一个文件中。如果使用传统的写操作这样做，客户端将需要额外复杂和昂贵的同步，例如通过分布式锁管理器。在我们的工作负载中，这样的文件经常用作多个生产者/单个消费者队列或包含来自许多不同客户端的合并结果。

记录追加是一种变异操作，并遵循第3.1节中的控制流，只需要在主处理器上进行一些额外的逻辑。客户端将数据推送到文件的最后一个块的所有副本，然后将其请求发送到主处理器。主处理器检查是否将记录追加到当前块会导致该块超过最大大小（64 MB）。如果是，它会将块填充到最大大小，告诉次要副本做同样的事情，并向客户端回复，指示应在下一个块上重试操作（为了保持最坏情况下的碎片化在可接受的水平上，记录追加被限制为最多是最大块大小的四分之一）。如果记录适合于最大大小（这是常见情况），主处理器将数据追加到其副本，告诉次要副本在确切的偏移量处写入数据，并最终向客户端回复成功。

如果记录追加在任何副本中失败，客户端将重试该操作。因此，相同块的副本可能包含不同的数据，可能包括完全或部分重复的同一记录。GFS不能保证所有副本是字节级别相同的。它只保证数据作为原子单位至少写入一次。这种属性很容易从简单的观察中得出：为了报告成功，数据必须在某个块的所有副本上都写入相同的偏移量。此外，此后，所有副本至少与记录的结尾一样长，因此任何未来的记录将被分配一个更高的偏移量或不同的块，即使稍后的不同副本成为主处理器。就我们的一致性保证而言，成功的记录追加操作写入其数据的区域是已定义的（因此是一致的），而介于这些区域之间的区域是不一致的（因此是未定义的）。我们的应用程序可以处理不一致的区域，正如我们在第2.7.2节中讨论的那样。
##### 3.5快照
GFS的快照操作允许在最小化对正在进行的变异的中断的同时，几乎瞬间地创建文件或目录树（“源”的）副本。我们的用户使用它来快速创建大型数据集的分支副本（通常是这些副本的副本，递归地），或在进行可以轻松提交或回滚的更改之前对当前状态进行检查点。

像AFS一样，我们使用标准的写时复制技术来实现快照。当主服务器接收到快照请求时，首先吊销任何在即将快照的文件中的块上的未处理租约。这确保对这些块的任何后续写操作都需要与主服务器进行交互以查找租约持有者。这将为主服务器提供首先创建块的新副本的机会。

在吊销或到期租约之后，主服务器将操作记录到磁盘中。然后，它通过复制源文件或目录树的元数据将此日志记录应用于其内存状态。新创建的快照文件指向与源文件相同的块。

在快照操作后，当客户端首次想要写入块C时，它会向主服务器发送请求以查找当前的租约持有者。主服务器注意到块C的引用计数大于1，推迟回复客户端请求，然后选择一个新的块句柄C'。然后，它要求每个当前拥有块C副本的块服务器创建一个名为C'的新块。通过在与原始块相同的块服务器上创建新块，我们确保数据可以在本地复制，而不是通过网络（我们的磁盘比100 Mb以太网链接快三倍）。从这一点开始，请求处理与任何块的处理方式没有区别：主服务器授予其中一个副本对新块C'的租约，并回复客户端，客户端可以正常写入块，不知道它刚刚是从现有块创建的。
#### 4.主服务器操作
主服务器执行所有命名空间操作。此外，它在整个系统中管理块副本：它做出放置决策，创建新块和因此创建副本，并协调各种系统范围的活动，以使块完全复制，平衡所有块服务器的负载，并回收未使用的存储空间。接下来我们将讨论这些主题。
##### 4.1命名空间管理和锁
许多主服务器操作可能需要很长时间，例如快照操作需要在快照覆盖的所有块上撤销块服务器租约。我们不希望在运行它们时延迟其他主服务器操作。因此，我们允许多个操作同时进行，并在命名空间的区域上使用锁来确保正确的序列化。与许多传统文件系统不同，GFS没有列出该目录中所有文件的每个目录数据结构。它也不支持相同文件或目录的别名（即Unix中的硬链接或符号链接）。 GFS逻辑上将其命名空间表示为将全路径名映射到元数据的查找表。使用前缀压缩，此表可以在内存中高效地表示。命名空间树中的每个节点（绝对文件名或绝对目录名）都有一个相关的读写锁。每个主服务器操作在运行之前都会获取一组锁。通常，如果它涉及到 /d1/d2/.../dn/leaf，则会获取目录名称 /d1、/d1/d2、...、/d1/d2/.../dn 上的读锁，并获取路径名 /d1/d2/.../dn/leaf 上的读锁或写锁。请注意，leaf 可能是文件或目录，具体取决于操作。现在我们来说明这种锁定机制如何防止在对 /home/user 进行快照到 /save/user 的过程中创建文件 /home/user/foo。快照操作获取 /home 和 /save 上的读锁，并获取 /home/user 和 /save/user 上的写锁。文件创建获取 /home 和 /home/user 上的读锁，以及 /home/user/foo 上的写锁。这两个操作将正确序列化，因为它们试图在 /home/user 上获取冲突锁。文件创建不需要在父目录上获取写锁，因为没有要受到保护以防止修改的 "目录" 或类似inode的数据结构。名称上的读锁足以保护父目录免受删除。这种锁定方案的一个好处是它允许在同一目录中进行并发变异。例如，在同一目录中可以同时执行多个文件创建：每个文件创建都会获取目录名称上的读锁和文件名称上的写锁。目录名称上的读锁足以防止删除、重命名或快照目录。文件名称上的写锁序列化试图两次创建具有相同名称的文件的尝试。由于命名空间可能具有许多节点，因此读写锁对象被延迟分配并在不使用时删除。此外，锁按一致的总顺序获取，以防止死锁：它们首先按命名空间树中的级别排序，然后按字典顺序排序。
##### 4.2副本放置
GFS集群在多个级别上高度分布。它通常拥有分布在许多机架上的数百个块服务器。这些块服务器反过来可以从来自同一或不同机架的数百个客户端访问。不同机架上的两台计算机之间的通信可能会跨越一个或多个网络交换机。此外，机架内的带宽可能小于机架内所有计算机的聚合带宽。多级分布对于实现可扩展性、可靠性和可用性的数据分布提出了独特的挑战。块副本放置策略有两个目的：最大化数据的可靠性和可用性，以及最大化网络带宽利用率。对于这两个目的，仅仅将副本分散在不同的计算机上是不够的，这只能防止磁盘或计算机故障，并充分利用每个计算机的网络带宽。我们还必须将块副本分散在不同的机架上。这样可以确保即使整个机架受损或离线（例如由于共享资源（如网络交换机或电源电路）的故障），某些块的副本仍将存活并保持可用。这也意味着一个块的流量，特别是读取流量，可以利用多个机架的聚合带宽。另一方面，写入流量必须流经多个机架，这是我们愿意做出的一种权衡。
##### 4.3创建、重新复制、重新平衡
块副本的创建有三个原因：块的创建、重新复制和再平衡。当主服务器创建块时，它选择在哪里放置最初为空的副本。它考虑了几个因素。第一，我们希望将新副本放置在磁盘空间利用率低于平均水平的块服务器上。随着时间的推移，这将使磁盘利用率在所有块服务器上均衡。第二，我们希望限制每个块服务器上“最近”创建的块数。虽然创建自身很便宜，但它可靠地预测到即将出现大量的写流量，因为块是在写入需求时创建的，在我们的“一次追加，多次读取”工作负载中，它们一旦被完全写入后通常变得几乎只读。第三，如上所述，我们希望在不同的机架上分散块的副本。

当可用副本数低于用户指定的目标时，主服务器立即重新复制块。这可能是由于多种原因导致的：块服务器不可用，它报告其副本可能已损坏，其中一个磁盘因错误而被禁用，或者副本目标增加。需要重新复制的每个块都根据几个因素进行优先级排序。其中一个是它距离其复制目标有多远。例如，我们将优先处理失去两个副本的块，而不是只失去一个副本的块。此外，我们更喜欢优先重新复制属于活动文件的块，而不是属于最近删除的文件的块（请参见第4.4节）。最后，为了最小化故障对运行应用程序的影响，我们提高了任何阻止客户端进度的块的优先级。

主服务器选择优先级最高的块，并通过指示某些块服务器直接从现有的有效副本中复制块数据来“克隆”它。新副本的目标与创建相似：均衡磁盘空间利用率，限制任何单个块服务器上的活动克隆操作，并在不同的机架上分散副本。为了避免克隆流量压倒客户端流量，主服务器限制了整个集群和每个块服务器上活动克隆操作的数量。此外，每个块服务器通过限制其到源块服务器的读请求来限制其在每个克隆操作上花费的带宽。

最后，主服务器会定期重新平衡副本：它检查当前的副本分布，并移动副本以实现更好的磁盘空间和负载均衡。通过这个过程，主服务器逐渐填满一个新的块服务器，而不是立即用新块和伴随而来的大量写入流量淹没它。新副本的放置标准与上面讨论的标准类似。此外，主服务器还必须选择要删除的现有副本。一般来说，它更喜欢删除磁盘空间利用率低于平均水平的块服务器上的副本，以均衡磁盘空间使用。
##### 4.4垃圾回收
在文件被删除后，GFS不会立即回收可用的物理存储空间。只有在定期的垃圾收集时，才会在文件和块级别上进行惰性回收。我们发现，这种方法使系统更加简单和可靠。
###### 4.4.1机制
当应用程序删除文件时，主服务器立即记录删除操作，就像其他更改一样。然而，与其立即回收资源不同，文件只是被重命名为包含删除时间戳的隐藏名称。在主服务器定期扫描文件系统命名空间时，如果这些隐藏文件已经存在超过三天（间隔可配置），它将删除任何这样的文件。在此之前，文件仍然可以在新的特殊名称下读取，并且可以通过将其重命名回常规名称来取消删除。当隐藏文件从命名空间中删除时，其内存中的元数据将被清除。这有效地切断了它与所有块的链接。

在块命名空间的类似定期扫描中，主服务器识别孤立的块（即那些无法从任何文件访问的块），并删除这些块的元数据。在与主服务器定期交换的 HeartBeat 消息中，每个块服务器报告其持有的一组块，主服务器回复不再存在于其元数据中的所有块的身份。块服务器可以自由地删除这些块的副本。
###### 4.4.2讨论
虽然在编程语言的上下文中进行分布式垃圾收集是一个需要复杂解决方案的难题，但在我们的情况下，这是相当简单的。我们可以轻松地识别所有对块的引用：它们在主服务器独占维护的文件到块映射中。我们还可以轻松地识别所有块副本：它们是每个块服务器上指定目录下的 Linux 文件。主服务器不知道的任何这样的副本都是“垃圾”。

垃圾收集方法提供了比急切删除更多的优点。首先，在组件故障常见的大规模分布式系统中，它是简单和可靠的。块创建可能在某些块服务器上成功，但在其他服务器上失败，留下主服务器不知道存在的副本。副本删除消息可能会丢失，主服务器必须记住在故障（包括自己和块服务器）后重新发送它们。垃圾收集提供了一种统一而可靠的方式来清除任何未知有用的副本。其次，它将存储回收合并到主服务器的定期后台活动中，例如命名空间的定期扫描和与块服务器的握手。因此，它是批处理完成的，成本是分摊的。此外，只有在主服务器相对空闲时才进行。主服务器可以更及时地响应需要及时关注的客户端请求。第三，延迟回收存储提供了防止意外不可逆删除的安全网。

根据我们的经验，主要的缺点是，当存储空间不足时，延迟有时会阻碍用户优化使用的努力。反复创建和删除临时文件的应用程序可能无法立即重用存储空间。我们通过加快存储回收，如果已删除的文件再次被显式删除，来解决这些问题。我们还允许用户对命名空间的不同部分应用不同的复制和回收策略。例如，用户可以指定某个目录树中文件中的所有块都不进行复制，并且任何删除的文件都会立即且不可撤销地从文件系统状态中删除。
###### 4.4.3陈旧副本检测
如果一个块服务器因故障而无法收到对块的变更操作，那么块副本可能会变得过时。对于每个块，主服务器都维护一个块版本号，以区分最新和过时的副本。每当主服务器授予一个块的新租约时，它会增加块版本号并通知最新的副本。主服务器和这些副本都在其持久状态中记录新的版本号。这发生在任何客户端被通知之前，因此在客户端开始写入块之前。如果另一个副本当前不可用，则其块版本号不会被提高。当块服务器重新启动并报告其块集及其相关版本号时，主服务器将检测到该块服务器有一个过时的副本。如果主服务器看到的版本号大于其记录中的版本号，则主服务器认为在授予租约时出现了故障，因此将较高版本视为最新的。

主服务器在其定期的垃圾收集中删除过时的副本。在此之前，当它回复客户端请求块信息时，它会将过时的副本视为不存在。作为另一个保障，当主服务器通知客户端哪个块服务器持有块的租约或指示块服务器从克隆操作中的另一个块服务器读取块时，它会包括块版本号。客户端或块服务器在执行操作时验证版本号，以确保始终访问最新的数据。
#### 5.容错和诊断
在设计系统时，我们面临的最大挑战之一是处理频繁的组件故障。组件的质量和数量共同使这些问题变得更加普遍而非例外：我们既不能完全信任机器，也不能完全信任磁盘。组件故障可能导致系统不可用，甚至更糟的情况是数据损坏。我们将讨论如何应对这些挑战以及将诊断问题的工具集成到系统中，以便在它们不可避免地发生时进行诊断。
##### 5.1高可用
在GFS集群中的数百台服务器中，随时可能有一些不可用。我们采用了两种简单而有效的策略来保持整个系统高度可用：快速恢复和复制。

###### 5.1.1 快速恢复
主服务器和块服务器都设计成可以在几秒钟内恢复其状态并启动，无论它们是如何终止的。实际上，我们不区分正常终止和异常终止；服务器通常只需通过终止进程来关闭。当客户端和其他服务器在等待请求时超时并重新连接到重新启动的服务器时，它们会遇到一些小问题，然后进行重试。第6.2.2节报告了观察到的启动时间。

###### 5.1.2 块复制
如前所述，每个块在不同机架上的多个块服务器上都有副本。用户可以为文件名空间的不同部分指定不同的复制级别。默认值为三。主服务器会根据需要克隆现有副本，以保持每个块在块服务器离线或通过校验和验证检测到损坏的副本时都完全复制（参见5.2节）。虽然复制对我们很有用，但我们正在探索其他形式的跨服务器冗余，例如对于我们逐渐增加的只读存储需求，采用奇偶校验或纠删码。我们预计，在我们的非常松散耦合的系统中实现这些更复杂的冗余方案是具有挑战性但是可行的，因为我们的流量主要是追加和读取，而不是小的随机写入。

###### 5.1.3 主服务器复制
主服务器状态进行复制以提高可靠性。其操作日志和检查点在多台机器上进行复制。只有在其日志记录已在本地磁盘和所有主服务器副本上刷新后，对状态的变更才被视为已提交。为简单起见，一个主服务器进程负责所有变更以及更改系统内部的垃圾回收等后台活动。当该主服务器失败时，它可以快速重新启动。如果它的机器或磁盘故障，GFS外部的监控基础设施将在其他地方启动一个新的主服务器进程，并使用复制的操作日志。客户端仅使用主服务器的规范名称（例如gfs-test），这是一个DNS别名，如果主服务器被迁移到另一台机器，则可以更改它。
此外，“影子”主服务器即使在主服务器宕机时也可以提供对文件系统的只读访问。它们是影子而不是镜像，因为它们可能会略微滞后于主服务器，通常只有几分之一秒。它们增强了未被积极改变的文件或不介意获得稍微陈旧结果的应用程序的读取可用性。实际上，由于文件内容是从块服务器中读取的，因此应用程序不会观察到陈旧的文件内容。在短时间窗口内可能陈旧的是文件元数据，例如目录内容或访问控制信息。

为了保持自身的信息同步，影子主服务器会读取正在增长的操作日志的副本，并按照与主服务器完全相同的顺序将更改应用于其数据结构。与主服务器一样，它在启动时轮询块服务器（以及之后不经常轮询）以查找块副本，并与它们频繁地进行握手消息交换以监视其状态。它仅依赖于主服务器，以获取主服务器决定创建和删除副本而产生的副本位置更新。
##### 5.2数据完整性
每个块服务器使用校验和来检测存储数据的损坏。鉴于GFS集群通常拥有数百台机器上的数千个磁盘，它经常会遇到磁盘故障，导致读写路径上的数据损坏或丢失（请参见第7节中的一个原因）。我们可以使用其他块副本恢复损坏的数据，但通过比较块服务器上的副本来检测损坏是不切实际的。此外，不同的副本可能是合法的：GFS变异的语义，特别是之前讨论的原子记录追加，不能保证相同的副本。因此，每个块服务器必须通过维护校验和来独立验证其自己的副本的完整性。

一个块被分成64 KB的块。每个块都有一个对应的32位校验和。与其他元数据一样，校验和保存在内存中，并与日志分开持久存储，与用户数据分开。

对于读取操作，在返回任何数据给请求者（无论是客户端还是另一个块服务器）之前，块服务器会验证与读取范围重叠的数据块的校验和。因此，块服务器不会将损坏的数据传播到其他计算机。如果块不与记录的校验和匹配，块服务器会向请求者返回一个错误，并向主服务器报告不匹配。作为响应，请求者将从其他副本中读取，而主服务器将从另一个副本克隆该块。在新的有效副本就位后，主服务器会指示报告不匹配的块服务器删除其副本。

校验和对读取性能的影响很小，原因如下。由于我们的大多数读操作跨越至少几个块，因此我们只需要读取和校验相对较小的额外数据来进行验证。GFS客户端代码通过尝试在校验和块边界上对齐读取来进一步减少这种开销。此外，块服务器上的校验和查找和比较是在没有任何I/O的情况下完成的，并且校验和计算通常可以与I/O重叠。

校验和计算针对追加到块末尾的写入进行了大量优化（而不是覆盖现有数据的写入），因为它们在我们的工作负载中占主导地位。我们只需要递增最后一个部分校验和块的校验和，并为追加填充的任何全新校验和块计算新的校验和。即使最后一个部分校验和块已经损坏并且我们现在无法检测到它，新的校验和值也不会与存储的数据匹配，当块下次读取时，将像往常一样检测到损坏。
相反，如果写入覆盖块的现有范围，我们必须读取和验证正在部分覆盖的范围的第一个和最后一个块，然后执行写入，最后计算和记录新的校验和。如果我们在部分覆盖它们之前不验证第一个和最后一个块，新的校验和可能会隐藏在未被覆盖的区域中存在的损坏。

在空闲期间，块服务器可以扫描和验证非活动块的内容。这使我们能够检测到很少读取的块中的损坏。一旦检测到损坏，主服务器可以创建一个新的未损坏的副本并删除已损坏的副本。这可以防止一个非活动但已损坏的块副本欺骗主服务器认为它有足够的有效块副本。
##### 5.3诊断工具
广泛而详细的诊断日志记录在问题隔离、调试和性能分析方面有着不可估量的帮助，而仅产生极小的成本。没有日志，很难理解机器之间的短暂、不可重复的交互。GFS 服务器生成记录许多重要事件（如 chunkservers 上下线）和所有 RPC 请求和回复的诊断日志。这些诊断日志可以自由删除，而不会影响系统的正确性。但是，我们尽可能保留这些日志，只要空间允许。

RPC 日志包括在网络上传输的确切请求和响应，除了正在读取或写入的文件数据。通过将请求与回复匹配并整理不同机器上的 RPC 记录，我们可以重建整个交互历史以诊断问题。这些日志还可以用作负载测试和性能分析的追踪。日志记录对性能的影响很小（且远远超过了收益），因为这些日志是按顺序和异步写入的。最近的事件也保存在内存中，并可用于持续的在线监控。

#### 6.测量

